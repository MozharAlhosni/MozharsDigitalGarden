- `Large Language Models` (`LLMs`) are `machine learning` models designed to process and analyze text. 
- `LLMs` are trained on massive amounts of data, and they learn the patterns in the language, allowing them to generate human-like responses to any query they receive.
- `LLMs` are normally based on very large deep `nerual networks`.
- `LLMs` applications include chatbots, language translation, and text summarization.
- `Embeddings` are representations of words and phrases in a high-dimensional space, and how they can be used to measure similarity between different pieces of text.
- `Attention` is a mechanism that allows models to focus on specific parts of the input during processing.
- The `transformer model architecture`, which is the backbone of many _state-of-the-art_ `language models`, and how it has revolutionized the field of `Natural Language Processing` (`NLP`).
- `Semantic search` is the process of understanding the meaning of a query and finding the most relevant results. `LLMs` can be used to perform `semantic search`.
