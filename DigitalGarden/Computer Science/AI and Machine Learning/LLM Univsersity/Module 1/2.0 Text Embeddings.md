- The quintessential task of `Natural Language Processing` (`NLP`) is to understand `human language`.
- An assignment of words to numbers is called a `word embedding`.
- `Word` embeddings don't only capture word similarity, but they also capture other properties of the language.
- A good `word embedding` would be able to capture not only properties such as "age" and "size", but also many more features of the word.
- One of the `Cohere embeddings` has `4096` `coordinates` associated with each word. These rows of `4096` `coordinates` are called `vectors`; therefore, `vectors` correspond to a `word`, and numbers inside a `vector` correspond to a `coordinate`.
- A `word embedding` can be seen as a good way of translating human language (`words`) into computer language (`numbers`), allowing `machine learning models` to train on them.

![[Word_Embedding.png]]

# Sentence Embeddings

- A `sentence embedding` is similar to a `word embedding`, except that it associates every `sentence` with a `vector` full of numbers in a `coherent` way, satisfying similar `properties` as a `word embedding`. That is: _similar_ `sentences` are assigned to _similar_ `vectors`, _different_ `sentences` are assigned _different_ `vectors`, and most importantly, each of the `coordinates` of the `vector` identifies some `property` of the sentence.

# Conclusion

- `Word` and `sentence` `embeddings` are the "bread and butter" of `LLMs`. They are the basic building blocks of most `language models`, since they translate human languages (`words`) to the computer language (`numbers`), capturing many relations/properties between `words`, `sematics`, and nuances of a language.
- `Sentence embeddings` can be extended to `language embeddings`, in which the numbers attached to each `sentence` are `language-agnostic`.